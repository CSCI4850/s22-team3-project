{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c9704b-bdcb-4248-9224-3cd94571a09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q \"tensorflow-text==2.8.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9405d31e-3b31-47d0-8cb0-776d721b301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 01:02:47.359385: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-03 01:02:47.362270: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc91b09-0a29-41f4-83b5-f813d0d65a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33eb7a0c-a52e-4b68-8238-d687c481274c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The important thing is not to stop questioning. Curiosity has its own reason for existence. One cannot help but be in awe when he contemplates the mysteries of eternity, of life, of the marvelous structure of reality. It is enough if one tries merely to comprehend a little of this mystery each day.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open data\n",
    "data_file = open('Data.txt').read()\n",
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9448eef1-72e2-488e-92b9-f824abd90d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create tokenizer object\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "#Convert data to lowercase\n",
    "data = data_file.lower().split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "597491c5-b43d-4e5e-a4c5-7e4c8eb764e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'of': 1, 'the': 2, 'is': 3, 'to': 4, 'one': 5, 'important': 6, 'thing': 7, 'not': 8, 'stop': 9, 'questioning': 10, 'curiosity': 11, 'has': 12, 'its': 13, 'own': 14, 'reason': 15, 'for': 16, 'existence': 17, 'cannot': 18, 'help': 19, 'but': 20, 'be': 21, 'in': 22, 'awe': 23, 'when': 24, 'he': 25, 'contemplates': 26, 'mysteries': 27, 'eternity': 28, 'life': 29, 'marvelous': 30, 'structure': 31, 'reality': 32, 'it': 33, 'enough': 34, 'if': 35, 'tries': 36, 'merely': 37, 'comprehend': 38, 'a': 39, 'little': 40, 'this': 41, 'mystery': 42, 'each': 43, 'day': 44} \n",
      "\n",
      "[[2, 6, 7, 3, 8, 4, 9, 10], [11, 12, 13, 14, 15, 16, 17], [5, 18, 19, 20, 21, 22, 23, 24, 25, 26, 2, 27, 1, 28, 1, 29, 1, 2, 30, 31, 1, 32], [33, 3, 34, 35, 5, 36, 37, 4, 38, 39, 40, 1, 41, 42, 43, 44], []] \n",
      "\n",
      "45 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create dictionary of words with the frequency they occur\n",
    "#Every word gets unique value > 0\n",
    "#0 is reserved for padding\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "#Transforms sentences into set of integers from the dictionary\n",
    "input_sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "#Counts total words\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "#Prints it all out\n",
    "print(tokenizer.word_index, '\\n')\n",
    "print(input_sequences, '\\n')\n",
    "print(total_words, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15c25582-4522-4342-b432-d95b67856fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6,  7,  3,  8,  4,  9, 10,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [11, 12, 13, 14, 15, 16, 17,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0],\n",
       "       [19, 20, 21, 22, 23, 24, 25, 26,  2, 27,  1, 28,  1, 29,  1,  2,\n",
       "        30, 31,  1, 32],\n",
       "       [33,  3, 34, 35,  5, 36, 37,  4, 38, 39, 40,  1, 41, 42, 43, 44,\n",
       "         0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pad sequences\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, maxlen = 20, padding ='post', value = 0)\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cfecd8e-06e8-4def-a00a-1a28b262cc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 20)\n",
      "[[ 2  6  7  3  8  4  9 10  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [11 12 13 14 15 16 17  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [19 20 21 22 23 24 25 26  2 27  1 28  1 29  1  2 30 31  1 32]\n",
      " [33  3 34 35  5 36 37  4 38 39 40  1 41 42 43 44  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "input_sequences = np.array(input_sequences)\n",
    "print(input_sequences.shape)\n",
    "print(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7632a936-695c-4b42-a829-25f1930c30ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 20)\n",
      "[[11 12 13 14 15 16 17  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [19 20 21 22 23 24 25 26  2 27  1 28  1 29  1  2 30 31  1 32]\n",
      " [33  3 34 35  5 36 37  4 38 39 40  1 41 42 43 44  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "#Shift array by one to create targets \n",
    "output_sequences = np.array(np.roll(input_sequences, 80))\n",
    "output_sequences[-1] = 0\n",
    "print(output_sequences.shape)\n",
    "print(output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f6eb672-9c0d-46d4-babc-c6ecfad50b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = input_sequences.shape[1:]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe46006c-1491-48d2-b31f-2979293d738d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_shape = output_sequences.shape[1:]\n",
    "output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c481ded-f750-4041-b4f2-f501864087f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 01:11:22.405207: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-03 01:11:22.405486: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-03 01:11:22.405670: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-gdm2z): /proc/driver/nvidia/version does not exist\n",
      "2022-04-03 01:11:22.408257: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "input_sequences = tf.convert_to_tensor(input_sequences)\n",
    "output_sequences = tf.convert_to_tensor(output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "872a0982-d581-4c4a-b375-d18d44b01a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 44 words (0 misses)\n"
     ]
    }
   ],
   "source": [
    "#prepare embedding matrix\n",
    "word_index = dict(zip(tokenizer.word_index, range(len(tokenizer.word_index))))\n",
    "num_tokens = len(tokenizer.word_index) + 2\n",
    "embedding_dim = 50\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e430908-0d7e-4143-9ebf-c0988839191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embedding_layer = keras.layers.Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c755f1c5-0316-48f7-9b3c-a61bc93a1812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.1657991e-03, -4.3009698e-02, -1.2794066e-02, ...,\n",
       "         4.0928017e-02, -2.0601345e-02, -1.2529600e-02],\n",
       "       [ 1.6522799e-02,  1.5623178e-02, -1.7693747e-02, ...,\n",
       "         4.0928017e-02, -2.0601345e-02, -1.2529600e-02],\n",
       "       [-4.2396512e-02,  4.2234566e-02,  3.9753143e-02, ...,\n",
       "         1.7466400e-02,  9.9800527e-05,  3.3993125e-03],\n",
       "       [ 4.2923320e-02, -2.9993057e-02, -2.7239729e-02, ...,\n",
       "         4.0928017e-02, -2.0601345e-02, -1.2529600e-02],\n",
       "       [ 3.9652433e-02,  4.9618069e-02,  3.1441823e-03, ...,\n",
       "         4.0928017e-02, -2.0601345e-02, -1.2529600e-02]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert to embedding vectors\n",
    "#embedding_model = keras.Sequential()\n",
    "\n",
    "input = keras.layers.Input(shape = input_sequences.shape)\n",
    "\n",
    "word_input = keras.layers.Input(shape = input_sequences.shape[1])\n",
    "\n",
    "embedding_layer = keras.layers.Embedding(input_dim = total_words, output_dim = 512, mask_zero = True, input_length = input_sequences.shape[1])(word_input) \n",
    "\n",
    "word_vectors = keras.layers.Flatten()(embedding_layer)\n",
    "\n",
    "embedding_model = keras.Model(word_input, word_vectors)\n",
    "\n",
    "embedding_model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(), metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "                    \n",
    "keras.utils.plot_model(embedding_model,to_file='model.png', show_shapes=True,expand_nested=True)\n",
    "\n",
    "word_vectors = embedding_model.predict(input_sequences)\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7a15e-785f-4ed3-8915-76a53ec45c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812e7f8-f002-4903-94e2-848e44cefc90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
