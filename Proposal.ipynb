{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0c8d31-16e9-45de-8059-f8b7aef31a8b",
   "metadata": {},
   "source": [
    "# Sigmoid Project Proposal\n",
    "__Authors: Kendra Givens, George Maddux, Kim Nguyen, Dennis Bandavong, Ankit Bhusal__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efda597-86b9-44e1-810c-22819f81edb0",
   "metadata": {},
   "source": [
    "__Overview__  \n",
    "The goal of this project is to create a generative chatbot based on Albert Einstein. He will be able to carry on a conversation indefinitely, keep the context of the conversation by remembering what was said in recent messages, and retain the memory of previous conversations he had with users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f4337-54c8-4827-8935-7f07c6391de0",
   "metadata": {},
   "source": [
    "__Background__  \n",
    "\tA chatbot is a computer program that simulates human conversation. The user will be provided with an interface to type messages to our program, which will respond in kind. There are two types of chatbots-generative (open domain) and retrieval (closed domain). A generative chatbot is one that will create responses rather than select one from a predetermined set, whereas a retrieval based bot will do the latter. For the purposes of this project we will be using a generative chatbot to create a more authentic experience for the user. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ac6d9-69ae-467d-9159-5e141ed6b068",
   "metadata": {},
   "source": [
    "__Architecture__  \n",
    "\tFor the neural network architecture, we will be using a deep neural network with transformer encoding and decoding. Transformer encoding models have self-attention layers that make advanced predictions based on current input. Self-attention will allow the chatbot to respond based more on the context of what is said because the words are weighted on proximity rather than order. Another model we considered using was seq2seq which is a recurrent neural network. We would have used it along with LSTM encoders and decoders, however, considering the size of the dataset we were using, we decided it would not have performed as well. Ultimately, we chose transformers over the seq2seq method to accomplish our goal because it requires less computation, does not run into the vanishing gradient problem, and can process in parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baae7faa-aa49-4f7e-8035-9d84d06fa535",
   "metadata": {},
   "source": [
    "__Dataset__  \n",
    "We will train our network on English translations of letters Einstein wrote, sourced from an online archive hosted by Princeton University as well as transcripts of interviews he gave. We specifically chose to use Einstein’s interview transcripts because it would mimic the language he would use to respond to questions. It is also a great measure of his intelligence, which we would like to replicate in the chat responses. Based on what Einstein had written in his letter, we were also looking to add his personality in the responses. The sentences will be processed according to the transformer encoder methods to produce vectors we will use to train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c475fd93-9435-4c5d-ab0d-a916cd00a645",
   "metadata": {},
   "source": [
    "__Validation__  \n",
    "\tFor assessing the performance of our model, we will conduct an Einstein Turing Test where we ask participants survey questions to assess how much they subjectively feel they are talking to Albert Einstein. We will also measure how human they feel the chatbot comes across and the quality of its conversation.\n",
    "\tAdditionally, we will search for a quantitative algorithm to measure the similarity between our chatbot’s responses and Einstein’s writing in his letters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17026d-4737-40cb-b396-048637fbdfb0",
   "metadata": {},
   "source": [
    "__Stretch Goals__  \n",
    "\tIf we have an accurate, working chatbot, we would like to add voice recognition to be able to converse with him. We are also considering adding his research papers as part of the dataset to expand the domain from casual to technical conversation. One more goal would be to update our Einstein model with modern science. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515d132-4809-40ea-ae3b-cab3075c6e14",
   "metadata": {},
   "source": [
    "__Citations__  \n",
    "Adaloglou, N. (2020, December 24). How transformers work in Deep Learning and NLP: An \n",
    "intuitive introduction. AI Summer. Retrieved March 3, 2022, from https://theaisummer.com/transformer/\n",
    "\n",
    "Myers, E. W. (n.d.). An O(ND) difference algorithm and its variations. CiteSeerX. Retrieved \n",
    "March 3, 2022, from http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.6927\n",
    "\n",
    "The Trustees of Princeton University. (n.d.). Digital Einstein Papers Home. Princeton University. \n",
    "Retrieved March 3, 2022, from https://einsteinpapers.press.princeton.edu/\n",
    "\n",
    "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & \n",
    "Polosukhin, I. (n.d.). Attention is all you need. Google. Retrieved March 3, 2022, from https://research.google.com/pubs/pub46201.html?source=post_page--------------------------\n",
    "\n",
    "What's the difference between attention vs self-attention? what problems does each other solve \n",
    "that the other can't? Data Science Stack Exchange. (1967, March 1). Retrieved March 3, 2022, from https://datascience.stackexchange.com/questions/49468/whats-the-difference-between-attention-vs-self-attention-what-problems-does-ea "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
